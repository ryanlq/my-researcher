"""
æµ‹è¯•æ··åˆ LLM é…ç½®
éªŒè¯ä¸åŒ LLM provider æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import os
from dotenv import load_dotenv

load_dotenv()

# æµ‹è¯•è„šæœ¬
async def test_mixed_llm():
    """æµ‹è¯•æ··åˆ LLM é…ç½®"""

    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•æ··åˆ LLM é…ç½®")
    print("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("\nğŸ“‹ å½“å‰é…ç½®:")
    print(f"  FAST_LLM:        {os.getenv('FAST_LLM')}")
    print(f"  SMART_LLM:       {os.getenv('SMART_LLM')}")
    print(f"  STRATEGIC_LLM:   {os.getenv('STRATEGIC_LLM')}")
    print(f"  EMBEDDING:       {os.getenv('EMBEDDING')}")
    print(f"  OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL')}")
    print(f"  OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL')}")

    # æµ‹è¯•å¯¼å…¥
    print("\nğŸ” æ£€æŸ¥ LLM providers:")

    # æµ‹è¯• Ollama
    try:
        if os.getenv("OLLAMA_BASE_URL"):
            import requests
            response = requests.get(f"{os.getenv('OLLAMA_BASE_URL')}/api/tags", timeout=2)
            if response.status_code == 200:
                print("  âœ… Ollama: å¯ç”¨")
                models = response.json().get("models", [])
                print(f"     å¯ç”¨æ¨¡å‹: {', '.join([m['name'] for m in models[:3]])}...")
            else:
                print("  âš ï¸  Ollama: æœåŠ¡æœªè¿è¡Œ")
        else:
            print("  âš ï¸  Ollama: æœªé…ç½®")
    except Exception as e:
        print(f"  âŒ Ollama: {e}")

    # æµ‹è¯•äº‘ç«¯ API
    try:
        if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_BASE_URL"):
            import requests
            headers = {"Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"}
            response = requests.get(f"{os.getenv('OPENAI_BASE_URL').rstrip('/v1')}/models", headers=headers, timeout=5)
            if response.status_code == 200:
                print("  âœ… SiliconFlow: å¯ç”¨")
            else:
                print(f"  âš ï¸  SiliconFlow: HTTP {response.status_code}")
        else:
            print("  âš ï¸  SiliconFlow: æœªé…ç½®")
    except Exception as e:
        print(f"  âŒ SiliconFlow: {e}")

    # æµ‹è¯• GPT Researcher é…ç½®
    print("\nğŸ”§ æµ‹è¯• GPT Researcher é…ç½®...")
    try:
        from gpt_researcher import GPTResearcher
        from gpt_researcher.config import Config

        config = Config()
        print(f"  âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"     - Fast LLM: {config.fast_llm_provider}:{config.fast_llm_model}")
        print(f"     - Smart LLM: {config.smart_llm_provider}:{config.smart_llm_model}")
        print(f"     - Strategic LLM: {config.strategic_llm_provider}:{config.strategic_llm_model}")
        print(f"     - Embedding: {config.embedding_provider}:{config.embedding_model}")

    except Exception as e:
        print(f"  âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")

    # å¿«é€ŸåŠŸèƒ½æµ‹è¯•
    print("\nğŸš€ å¿«é€ŸåŠŸèƒ½æµ‹è¯•...")
    try:
        import mcp_result_patch
        from gpt_researcher import GPTResearcher

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ç ”ç©¶å™¨è¿›è¡Œæµ‹è¯•
        researcher = GPTResearcher(
            query="æµ‹è¯•æŸ¥è¯¢",
            report_type="resource_report",
            verbose=False
        )

        print("  âœ… GPTResearcher åˆå§‹åŒ–æˆåŠŸ")
        print("  âœ… æ··åˆ LLM é…ç½®å¯ä»¥æ­£å¸¸å·¥ä½œ")

    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 60)
    print("ğŸ“ é…ç½®å»ºè®®:")
    print("=" * 60)
    print("""
æ··åˆé…ç½®ç­–ç•¥:
1. FAST_LLM (å¿«é€Ÿä»»åŠ¡)     â†’ ä½¿ç”¨æœ¬åœ° Ollama (èŠ‚çœæˆæœ¬)
2. SMART_LLM (å¤æ‚ä»»åŠ¡)    â†’ ä½¿ç”¨äº‘ç«¯æ¨¡å‹ (è´¨é‡ä¼˜å…ˆ)
3. STRATEGIC_LLM (è§„åˆ’)    â†’ æ ¹æ®éœ€æ±‚é€‰æ‹©
4. EMBEDDING (åµŒå…¥)        â†’ ä½¿ç”¨æœ¬åœ°æ¨¡å‹ (èŠ‚çœ API è°ƒç”¨)

æˆæœ¬ä¼˜åŒ–:
- æœ¬åœ° Ollama: å…è´¹
- äº‘ç«¯ API: æŒ‰ä½¿ç”¨é‡ä»˜è´¹
- å»ºè®®å°† 60-80% çš„ä»»åŠ¡åˆ†é…ç»™æœ¬åœ°æ¨¡å‹

æ€§èƒ½ä¼˜åŒ–:
- Ollama å“åº”é€Ÿåº¦: 2-5 ç§’
- äº‘ç«¯ API å“åº”é€Ÿåº¦: 1-3 ç§’
- æ··åˆä½¿ç”¨å¯ä»¥å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
    """)


if __name__ == "__main__":
    asyncio.run(test_mixed_llm())
